{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715b058e",
   "metadata": {},
   "source": [
    "# Titanic Project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edfe456",
   "metadata": {},
   "source": [
    "## Prelude:\n",
    "    In this project we are given data regarding the passengers travelling in the iconic ship called \"Titanic\". Given features and label attributes tells us about the particulars of the passengers travelling in the ship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99e84b",
   "metadata": {},
   "source": [
    "Let us now get familiar with the feature variables:\n",
    "\n",
    "1. PassengerId: It is an identifying unique numerical variable\n",
    "\n",
    "2. Pclass: This is a catergorical numeric variable with 3 categories which refers to socio-economic class of the passenger\n",
    "\n",
    "3. Name: This is an identifying character variable\n",
    "\n",
    "4. Sex: This is a categorical variable with 2 categories describing the gender of the passenger\n",
    "\n",
    "5. Age: This is a numeric variable\n",
    "\n",
    "6. Sibsp: This is a numeric categorical variable with 7 categories describing about the number of sibling passenger has got\n",
    "\n",
    "7. Parch: This is a numeric categorical variable with 7 categories describing the parents and number of siblings of a passenger\n",
    "\n",
    "8. Ticket: This is an identifying character variable \n",
    "\n",
    "9. Fare: This is a numeric variable describing the price of ticket purchased in GBP\n",
    "\n",
    "10. Cabin: This is a categorical variable describing about the cabin \n",
    "\n",
    "11. Embark: This is a character variable describing about the port from which port the passengers boarded the ship\n",
    "     (C = Cherbourg; Q = Queenstown; S = Southampton) \n",
    "     \n",
    "Moving forward let's throw some light on our label variable\n",
    "Survived: This is a categorical variable with two categories. (0= Not Survived, 1= Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e71ca",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "   ####  We have to build a classification machine learning model to predict the survival of the passenger boarding the ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d77af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "\n",
    "# Importing fundamental packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.core.interactiveshell import InteractiveShell        ## To display multiple outputs\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pyforest            \n",
    "\n",
    "## For visualization\n",
    "import matplotlib.pyplot as plt                                         \n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "## Data Pre-Processing Packages\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.stats import zscore\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "## To create copy of data\n",
    "import copy\n",
    "\n",
    "## Pipeline Packages\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "## Ensemble Learning Algorithms Packages\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "## Evaluation Metrics Packages\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix, accuracy_score, classification_report, plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Bagging and Boosting\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier                           \n",
    "\n",
    "# Saving the model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c6965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(link):\n",
    "    global data\n",
    "\n",
    "    data=pd.read_csv(link)\n",
    "    \n",
    "    data=pd.DataFrame(data)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3adfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "..           ...       ...     ...   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket     Fare Cabin Embarked  \n",
      "0        0         A/5 21171   7.2500   NaN        S  \n",
      "1        0          PC 17599  71.2833   C85        C  \n",
      "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3        0            113803  53.1000  C123        S  \n",
      "4        0            373450   8.0500   NaN        S  \n",
      "..     ...               ...      ...   ...      ...  \n",
      "886      0            211536  13.0000   NaN        S  \n",
      "887      0            112053  30.0000   B42        S  \n",
      "888      2        W./C. 6607  23.4500   NaN        S  \n",
      "889      0            111369  30.0000  C148        C  \n",
      "890      0            370376   7.7500   NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "read(link=\"titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1baa008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(mydata):                                    # Defining function\n",
    "    \n",
    "    pd.set_option(\"display.max_rows\", None)         # to display all rows\n",
    "    pd.set_option(\"display.max_columns\", None)       #to display all columns\n",
    "    \n",
    "    print(mydata.head())                              # to display first 10 records\n",
    "    print(\"\\n\")                               \n",
    "    print(mydata.tail())                              # to display last 10 records\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(mydata.info())                               # to understand attributes of the data\n",
    "    \n",
    "    print(data.describe())                          # to get descriptive statistics\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Skewness for the data\",\"\\n\",data.skew())       # to get skewness of the data, skewness=0 for normal distribution\n",
    "    print(\"\\n\")\n",
    "    print(\"Kurosis for the data\",\"\\n\",data.kurtosis() )            # to get kutosis, kurtosis <=3 for normal distribution\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    sns.pairplot(mydata, kind='scatter', diag_kind='kde')                       # to represent data graphically\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")    \n",
    "    \n",
    "    plt.figure(figsize=(10,10))                      # plotting heat map to check correlation\n",
    "    sns.heatmap(mydata.corr(method = \"pearson\"), annot = True)\n",
    "    print(\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487d1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "\n",
      "     PassengerId  Survived  Pclass                                      Name  \\\n",
      "886          887         0       2                     Montvila, Rev. Juozas   \n",
      "887          888         1       1              Graham, Miss. Margaret Edith   \n",
      "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
      "889          890         1       1                     Behr, Mr. Karl Howell   \n",
      "890          891         0       3                       Dooley, Mr. Patrick   \n",
      "\n",
      "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
      "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
      "887  female  19.0      0      0      112053  30.00   B42        S  \n",
      "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
      "889    male  26.0      0      0      111369  30.00  C148        C  \n",
      "890    male  32.0      0      0      370376   7.75   NaN        Q  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skewness for the data \n",
      " PassengerId    0.000000\n",
      "Survived       0.478523\n",
      "Pclass        -0.630548\n",
      "Age            0.389108\n",
      "SibSp          3.695352\n",
      "Parch          2.749117\n",
      "Fare           4.787317\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Kurosis for the data \n",
      " PassengerId    -1.200000\n",
      "Survived       -1.775005\n",
      "Pclass         -1.280015\n",
      "Age             0.178274\n",
      "SibSp          17.880420\n",
      "Parch           9.778125\n",
      "Fare           33.398141\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eda(mydata=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a data copy to understand significant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee1de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1= copy.deepcopy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48cafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Cabin']=data1['Cabin'].astype(str).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "data1[[\"Cabin\"]] = enc.fit_transform(data1[[\"Cabin\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2204905",
   "metadata": {},
   "source": [
    "Conducting a chi-square to test the significance of the variable \"Cabin\" to label variable \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07083ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(data1[\"Cabin\"], data1[\"Survived\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0= Cabin does not impact survival of a passenger\n",
    "# HA= Cabin does impact survival of a passenger\n",
    "\n",
    "# defining the table\n",
    "stat, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "# interpret p-value\n",
    "alpha = 0.05\n",
    "print(\"p value is \" , str(p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84055256",
   "metadata": {},
   "source": [
    "Conducting a chi-square to test the significance of the variable \"Sex\" to label variable \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(data1[\"Sex\"], data1[\"Survived\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0= Sex does not impact survival of a passenger\n",
    "# HA= Sex does impact survival of a passenger\n",
    "\n",
    "# defining the table\n",
    "stat, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "# interpret p-value\n",
    "alpha = 0.05\n",
    "print(\"p value is \" , str(p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110578eb",
   "metadata": {},
   "source": [
    "Conducting a chi-square to test the significance of the variable \"Age\" to label variable \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(data1[\"Age\"], data1[\"Survived\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0= Age does not impact survival of a passenger\n",
    "# HA= Age does impact survival of a passenger\n",
    "\n",
    "# defining the table\n",
    "stat, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "# interpret p-value\n",
    "alpha = 0.05\n",
    "print(\"p value is \" , str(p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e966bfcd",
   "metadata": {},
   "source": [
    "Conducting a chi-square to test the significance of the variable \"Name\" to label variable \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(data1[\"Name\"], data1[\"Survived\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1cf151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0= Name does not impact survival of a passenger\n",
    "# HA= Name does impact survival of a passenger\n",
    "\n",
    "# defining the table\n",
    "stat, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "# interpret p-value\n",
    "alpha = 0.05\n",
    "print(\"p value is \" , str(p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9cb0d",
   "metadata": {},
   "source": [
    "Conducting a chi-square to test the significance of the variable \"PassengerId\" to label variable \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f69036",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(data1[\"PassengerId\"], data1[\"Survived\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0= Passenger ID does not impact survival of a passenger\n",
    "# HA= Passenger Id does impact survival of a passenger\n",
    "\n",
    "# defining the table\n",
    "stat, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "# interpret p-value\n",
    "alpha = 0.05\n",
    "print(\"p value is \" , str(p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c23d37",
   "metadata": {},
   "source": [
    "Conducting a chi-square to test the significance of the variable \"Embarked\" to label variable \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8833712",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(data1[\"Embarked\"], data1[\"Survived\"])\n",
    "# H0= Passenger Id does not impact survival of a passenger\n",
    "# HA= Passenger Id does impact survival of a passenger\n",
    "\n",
    "# defining the table\n",
    "stat, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "# interpret p-value\n",
    "alpha = 0.05\n",
    "print(\"p value is \" , str(p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ded77",
   "metadata": {},
   "source": [
    "Conducting a chi-square to test the significance of the variable \"Ticket\" to label variable \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32451813",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(data1[\"Ticket\"], data1[\"Survived\"])\n",
    "# H0= Ticket number does not impact survival of a passenger\n",
    "# HA= Ticket number  does impact survival of a passenger\n",
    "\n",
    "# defining the table\n",
    "stat, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "# interpret p-value\n",
    "alpha = 0.05\n",
    "print(\"p value is \" , str(p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(data1[\"Pclass\"], data1[\"Survived\"])\n",
    "# H0= class does not impact survival of a passenger\n",
    "# HA= class  does impact survival of a passenger\n",
    "\n",
    "# defining the table\n",
    "stat, p, dof, expected = chi2_contingency(crosstab)\n",
    "\n",
    "# interpret p-value\n",
    "alpha = 0.05\n",
    "print(\"p value is \" , str(p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier(mydata):                        # Outlier Plotting\n",
    "    for i in mydata.columns:\n",
    "        fig = px.box(mydata, y= i, width=600, height=400, title=i, template=\"plotly_dark\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ec96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier(mydata=data.drop([\"Survived\"],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram for all the variables using plotly_express by 'quality'\n",
    "\n",
    "for i in data.columns:\n",
    "    fig = px.histogram(data, x= i, histfunc = \"count\", color = \"Survived\", \n",
    "                       width=1000, height=800, title = \"Histogram for \" + i, \n",
    "                       template=\"plotly_dark\")\n",
    "\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08fa7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percntage of Survival found in Titanic\n",
    "percentage=(data['Survived'].value_counts())*100 / (len(data['Survived']))\n",
    "print(percentage.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e7a95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.groupby([\"Survived\"]).describe().transpose() ### Five point summary grouped by \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ce197",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Survived\"].value_counts(ascending=True)     # Finding values in each class of Survival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679640fa",
   "metadata": {},
   "source": [
    "## Findings:  \n",
    "\n",
    "01. There are 891 rows and 12 columns\n",
    "\n",
    "02. There are missing values in the data\n",
    "\n",
    "03. There are 7 numeric variables and 5 object variables\n",
    "\n",
    "04. There's no multicollinearity in the data\n",
    "\n",
    "05. Variables like \"Passenger Id\", \"Pclass\", \"Name\" and \"Age\" are not significant for predicting survival of a passenger,           hence, it can be dropped from our data\n",
    "\n",
    "06. Out of 891 passengers, around 342 (38.38%) passengers survived. In survivors maximum were women and children.\n",
    "\n",
    "07. Average age of passengers who boarded is 29.699 years and passengers who survived is 28.343 years\n",
    "\n",
    "08. Average price of Tickets purchased was GBP 32.20 and for passengers survived it was GBP 48.4\n",
    "\n",
    "09. Considering the price of ticket we see survived passengerswere socio-economically upper class\n",
    "\n",
    "10. Generally, passengers boarded the ship belonged to Middle-class socio-economically.\n",
    "\n",
    "11. From our various chi-square tests we find the survival chances of the passengers is influenced by their age, sex,               embarkment and socio economic class as higher the price of ticket better is their survival chances.\n",
    "\n",
    "12. In this dataset, we find variables like Parch, Fare, Sibsp and Age have outliers.\n",
    "\n",
    "13. Variables like \"SibSp\",\"Parch\" and \"Fare\" have outliers\n",
    "\n",
    "14. Maximum Survivors had boarded the ship from Cherbourg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37857e43",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of dataset for cleaning and model building purpose\n",
    "\n",
    "data2= copy.deepcopy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36356244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping insignificant columns\n",
    "data2= data.drop([\"PassengerId\",\"Pclass\",\"Name\",\"Age\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e6495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Int variable to Float variable for column Age\n",
    "data2['Age']=data['Age'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d97b5e",
   "metadata": {},
   "source": [
    "Encoding the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding \"Cabin\" variable\n",
    "\n",
    "data2['Cabin']=data2['Cabin'].astype(str).str[0]\n",
    "enc = OrdinalEncoder()\n",
    "data2[[\"Cabin\"]] = enc.fit_transform(data2[[\"Cabin\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding \"Sex\" and \"Embarked\" variables\n",
    "enc2= LabelEncoder()\n",
    "data2[[\"Sex\",\"Embarked\"]]=data2[['Sex', 'Embarked']].apply(enc2.fit_transform)\n",
    "\n",
    "# Embark: Cherbourg=0 Queenstown=1 Southampton=2\n",
    "# Sex: Male= 0, Female= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only numeric for variable \"Ticket\"\n",
    "data2[\"Ticket\"]=data2[\"Ticket\"].str.extract('(\\d+)').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702241b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test\n",
    "\n",
    "def split (data,target):\n",
    "    data_reset_index = data.reset_index(drop=True)\n",
    "# Data split\n",
    "    global x\n",
    "    global y\n",
    "    global x_train\n",
    "    global y_train\n",
    "    global x_test\n",
    "    global y_test\n",
    "# Segregate Feature & Target Variables\n",
    "    x = data_reset_index.drop(target, axis=1)\n",
    "    y = data_reset_index[target]\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3, random_state=3)\n",
    "    \n",
    "    print(x_train.info())\n",
    "    (\"\\n\")\n",
    "    print(x_test.info())\n",
    "    (\"\\n\")\n",
    "    print(y_train.shape)\n",
    "    (\"\\n\")\n",
    "    print(y_test.shape)\n",
    "    (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f9091",
   "metadata": {},
   "outputs": [],
   "source": [
    "split(data=data2,\n",
    "      target=\"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8284ae3",
   "metadata": {},
   "source": [
    "### Imputing Missing Values in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f0b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_1= SimpleImputer(strategy= \"mean\")\n",
    "impute_2= IterativeImputer(max_iter=10, random_state= 0)\n",
    "impute_3= KNNImputer(n_neighbors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcec922",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('impute',impute_1),\n",
    "    ('model',model),\n",
    "])    \n",
    "pipe2 = Pipeline([\n",
    "    ('impute',impute_2),\n",
    "    ('model',model),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('impute',impute_3),\n",
    "    ('model',model),  \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e1b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Function pre_process to check the score for strategy\n",
    "def pre_process(data, pipe):\n",
    "    \n",
    "    data_reset_index = data.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "# Pipe.fit, pipe.predict and accuracy\n",
    "    \n",
    "    pipe.fit(x_train,y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(x_test)\n",
    "    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_pred, y_test))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523603e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(data= data2,\n",
    "           pipe= pipe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28436e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(data= data2,\n",
    "            pipe= pipe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(data= data2,\n",
    "            pipe= pipe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(feature, ft_train, ft_test, impute, target, target_train, target_test):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Imputing the training dataset with final chosen imputation method\n",
    "    \n",
    "    # converting array into dataframe\n",
    "    \n",
    "    ft_train = pd.DataFrame(impute.fit_transform(ft_train))\n",
    "    \n",
    "    # Assigning column names to the training data\n",
    "    \n",
    "    ft_train.columns = feature.columns\n",
    "    \n",
    "    print(\"Final Imputed Training Feature Data Set: \\n\", ft_train.isnull().sum())\n",
    "    \n",
    "    print(\" \")\n",
    "    \n",
    "    target_train = pd.DataFrame(np.array(target_train))\n",
    "    target_train.columns = [target]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Creating final training dataset (concatenating feature training and target training)\n",
    "    \n",
    "    global final_train\n",
    "    \n",
    "    final_train = pd.concat([ft_train, target_train], axis = 1)\n",
    "    \n",
    "    print(\"Final Train Data Health: \")\n",
    "    \n",
    "    print(\" \")\n",
    "    \n",
    "    print(final_train.info())\n",
    "    \n",
    "    print(\" \")\n",
    "    \n",
    "        \n",
    "    print(\"Final look: \\n\", final_train.isnull().sum())\n",
    "    \n",
    "    print(\" \")\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Imputing the test dataset with final chosen imputation method\n",
    "    \n",
    "    # converting array into dataframe\n",
    "    \n",
    "    ft_test = pd.DataFrame(impute.fit_transform(ft_test))\n",
    "    \n",
    "    # Assigning column names to the test data\n",
    "    \n",
    "    ft_test.columns = feature.columns\n",
    "    \n",
    "    print(\"Final Imputed Test Feature Data Set: \\n\", ft_test.isnull().sum())\n",
    "    \n",
    "    print(\" \")\n",
    "    \n",
    "    target_test = pd.DataFrame(np.array(target_test))\n",
    "    \n",
    "    target_test.columns = [target]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Creating final test dataset (concatenating feature training and target training)\n",
    "    \n",
    "    \n",
    "    \n",
    "    global final_test\n",
    "    \n",
    "    final_test = pd.concat([ft_test, target_test], axis = 1)\n",
    "    \n",
    "    print(\"Final Test Data Health: \")\n",
    "    \n",
    "    print(\" \")\n",
    "    \n",
    "    print(final_test.info())\n",
    "    \n",
    "    print(\" \")\n",
    "    \n",
    "   \n",
    "    \n",
    "    print(\"Final look: \\n\", final_test.isnull().sum())\n",
    "    \n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ddab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute(feature = x , \n",
    "       ft_train = x_train , \n",
    "       ft_test = x_test , \n",
    "       impute = impute_3 ,\n",
    "       target = 'Survived',\n",
    "       target_train = y_train, \n",
    "       target_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4613100",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=pd.concat([final_train,final_test ], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2786cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda(mydata=data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b69f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier(mydata=data3.drop([\"Survived\"],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram for all the variables using plotly_express by 'quality'\n",
    "\n",
    "for i in data3.columns:\n",
    "    fig = px.histogram(data2, x= i, histfunc = \"count\", color = \"Survived\", \n",
    "                       width=1000, height=800, title = \"Histogram for \" + i, \n",
    "                       template=\"plotly_dark\")\n",
    "\n",
    "    fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ec36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting qq plot\n",
    "\n",
    "for i in data3.columns:\n",
    "    fig = sm.qqplot(data2[i])\n",
    "\n",
    "    fig.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percntage of quality found in wine\n",
    "percentage=(data3['Survived'].value_counts())*100 / (len(data2['Survived']))\n",
    "print(percentage.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e80111",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data3.drop(['Survived'],axis=1).columns:\n",
    "    data3[i] = np.where(data3[i] > (data3[i].quantile(0.75) + (data3[i].quantile(0.75) - data3[i].quantile(0.25))*1.5),\n",
    "                           (data3[i].quantile(0.75) + (data3[i].quantile(0.75) - data3[i].quantile(0.25))*1.5),\n",
    "                          np.where(data3[i] < (data3[i].quantile(0.25) - (data3[i].quantile(0.75) - data3[i].quantile(0.25))*1.5),\n",
    "                           (data3[i].quantile(0.25) - (data3[i].quantile(0.75) - data3[i].quantile(0.25))*1.5),data3[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487fe1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier(mydata=data2.drop([\"Survived\"],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd5819",
   "metadata": {},
   "source": [
    "Now we have cleaned our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3[\"Survived\"].value_counts(abs,ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eda(mydata=data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ffce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.groupby([\"Survived\"]).describe().transpose() ### Five point summary grouped by \"Survived\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81d7ef",
   "metadata": {},
   "source": [
    "## Findings:  \n",
    "\n",
    "01. There are 891 rows and 9 columns\n",
    "\n",
    "02. There are no missing values in the data\n",
    "\n",
    "03. There are no object variables\n",
    "\n",
    "04. There's no multicollinearity in the data\n",
    "\n",
    "05. Variables like \"Passenger Id\", \"Pclass\", \"Name\" and \"Age\" are not significant for predicting survival of a passenger,           hence, are dropped from our data\n",
    "\n",
    "06. Out of 891 passengers, around 342 (38.38%) passengers survived. In survivors maximum were women and children.\n",
    "\n",
    "07. Average age of passengers who boarded is 29.699 years and passengers who survived is 28.558 years \n",
    "\n",
    "08. Average price of Tickets purchased was GBP 32.20 and for passengers survived it was GBP 48.4\n",
    "\n",
    "09. Considering the price of ticket we see survived passengerswere socio-economically upper class\n",
    "\n",
    "10. Generally, passengers boarded the ship belonged to Middle-class socio-economically.\n",
    "\n",
    "11. In this dataset, we find variables like Parch, Fare, Sibsp and Age have outliers.\n",
    "\n",
    "12. Maximum Survivors had boarded the ship from Cherbourg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9880fa",
   "metadata": {},
   "source": [
    "From the above values we see a problem of class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef70637",
   "metadata": {},
   "outputs": [],
   "source": [
    "split(data=data3,\n",
    "      target=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb46604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying Smote\n",
    "sm = SMOTE(random_state = 5)\n",
    "\n",
    "columns = x_train.columns\n",
    "\n",
    "train_data = pd.concat([x_train,y_train], axis = 1)\n",
    "\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_os_train, y_os_train  = sm.fit_resample(train_data.drop('Survived', axis = 1), train_data['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_os_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of records in Oversampled Train dataset\n",
    "\n",
    "print('the number of records in x_os_train :', len(x_os_train))\n",
    "print('the number of records in y_os_train :', len(y_os_train))\n",
    "print('the number of records in x_test :', len(x_test))\n",
    "print('the number of records in y_test :', len(y_test))\n",
    "\n",
    "# Target Class Distribution for Train and test Dataset\n",
    "\n",
    "print('the ratio of 0 and 1 in y_os_train:')\n",
    "\n",
    "print(y_os_train.value_counts(normalize = True)*100)\n",
    "\n",
    "print('the ratio of 0 and 1 in y_test:')\n",
    "\n",
    "print(y_test.value_counts(normalize = True)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before UpSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before UpSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf07ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After UpSampling, counts of label '1': {}\".format(sum(y_os_train==1)))\n",
    "print(\"After UpSampling, counts of label '0': {} \\n\".format(sum(y_os_train==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61452275",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed1192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_1= StandardScaler()\n",
    "scale_2= MinMaxScaler()\n",
    "scale_3= RobustScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020852d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([\n",
    "    ('Scale',scale_1),\n",
    "    ('model',model),\n",
    "])    \n",
    "pipe2 = Pipeline([\n",
    "    ('Scale',scale_2),\n",
    "    ('model',model),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('Scale',scale_3),\n",
    "    ('model',model),  \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48131706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Function Name \n",
    "def pre_process(data, pipe):\n",
    "\n",
    "# Pipe.fit, pipe.predict and accuracy\n",
    "    \n",
    "    pipe.fit(x_os_train,y_os_train)\n",
    "    \n",
    "    y_pred = pipe.predict(x_test)\n",
    "    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_pred, y_test))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f38bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(data= data3,\n",
    "           pipe= pipe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d19694",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(data= data3,\n",
    "           pipe= pipe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(data= data3,\n",
    "           pipe= pipe3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ce9d6",
   "metadata": {},
   "source": [
    "Here we see all techniques are providing same score. We can use standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar= StandardScaler()\n",
    "scalar.fit(x_train)\n",
    "x_trainsc =  scalar.transform(x_os_train)\n",
    "x_testsc  =  scalar.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f221d4e0",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fdfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.5)\n",
    "pca.fit(x_trainsc)\n",
    "x_train_model = pca.transform(x_trainsc)\n",
    "x_test_model = pca.transform(x_testsc)\n",
    "ex_variance=np.var(x_train_model,axis=0)\n",
    "ex_variance_ratio = ex_variance/np.sum(ex_variance)\n",
    "\n",
    "print(\"shape of x_train_pca\", x_train_model.shape)\n",
    "print('')    \n",
    "print(\"Explained Variance Ratio for Training Dataset: \\n\", ex_variance_ratio)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "ex_variance_1 = np.var(x_test_model , axis=0)\n",
    "ex_variance_ratio_1 = ex_variance_1 / np.sum(ex_variance_1)\n",
    "    \n",
    "print(\"Explained Variance Ratio for Test Dataset: \\n\", ex_variance_ratio_1) \n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae908a2",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28ba53",
   "metadata": {},
   "source": [
    "### Naive Baye's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model          =      GaussianNB()\n",
    "\n",
    "## Model.fit\n",
    "\n",
    "nb_model.fit(x_trainsc, y_os_train)\n",
    "\n",
    "## Model.predict\n",
    "\n",
    "\n",
    "y_pred_nb_0 = nb_model.predict(x_testsc)\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb_0, labels=[0, 1])\n",
    "\n",
    "df_cmatrix_nb = pd.DataFrame(conf_matrix_nb, index = [i for i in [0, 1]],\n",
    "                  columns = [i for i in [\"Predict_Rejected\",\"Predict_Accepted\"]])\n",
    "\n",
    "\n",
    "fig_cmatrix_nb = px.imshow(df_cmatrix_nb , title = \"Confusion Matrix for NB Classifier Model\")\n",
    "\n",
    "## Saving the Classification Reports : precision, recall, f1-score ##\n",
    "\n",
    "\n",
    "pred_report_nb = classification_report(y_test, y_pred_nb_0 , digits=2)\n",
    "\n",
    "### NB Classifier\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,  y_pred_nb_0)\n",
    "roc_nb = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "\n",
    "print(\"The Accuracy Score For The NB Classifier Model Is :  \", accuracy_score(y_test, y_pred_nb_0))\n",
    "print(\"\\n\")\n",
    "print(\"The roc_auc score for NB Classifier Model:  \", roc_nb)\n",
    "\n",
    "\n",
    "\n",
    "##  Confusion Matrices \n",
    "\n",
    "\n",
    "fig_cmatrix_nb.show()\n",
    "\n",
    "### Classification Reports \n",
    "\n",
    "print(\"Title : The Classification Report for NB Classifier Model: \\n  \", pred_report_nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K Fold Cross Validation ####\n",
    "    \n",
    "cross_valid = True\n",
    "cv_score = cross_val_score(nb_model , x_trainsc , y_os_train, scoring = 'accuracy', \n",
    "                                          cv = KFold(n_splits = 10))\n",
    "accuracy = accuracy_score(y_test, y_pred_nb_0)\n",
    "\n",
    "print(\"The Cross Validation Score For Baseline Support Vector Classifier Model After\", \"Fold Cross Validation Is :  \\n\", cv_score)\n",
    "print(\" \")\n",
    "print(\"The Accuracy Score For Baseline Support Vector Classifier Model After Cross Validation Is: \\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fe733",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8005b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "lda_model         =      LinearDiscriminantAnalysis()\n",
    "\n",
    "## Model.fit\n",
    "\n",
    "lda_model.fit(x_trainsc, y_os_train)\n",
    "\n",
    "## Model.predict\n",
    "\n",
    "y_lda_0 = lda_model.predict(x_testsc)\n",
    "\n",
    "## Linear Discriminant Analysis Model\n",
    "\n",
    "\n",
    "conf_matrix_lda =  confusion_matrix(y_test, y_lda_0 , labels=[0, 1])\n",
    "\n",
    "df_cmatrix_lda   = pd.DataFrame(conf_matrix_lda , index = [i for i in [0, 1]],\n",
    "                  columns = [i for i in [\"Predict_Rejected\",\"Predict_Accepted\"]])\n",
    "\n",
    "\n",
    "fig_cmatrix_lda = px.imshow(df_cmatrix_lda , title = \"Confusion Matrix for Baseline Linear Discriminant Analysis Model\")\n",
    "\n",
    "## Saving the Classification Reports : precision, recall, f1-score ##\n",
    "\n",
    "pred_report_lda = classification_report(y_test, y_lda_0 , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_lda_0)\n",
    "roc_lda = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "#### Printing the Accuracy Scores \n",
    "\n",
    "print(\"The Accuracy Score For The Linear Discriminant Analysis Model Is :  \",   \n",
    "      accuracy_score(y_test, y_lda_0))\n",
    "\n",
    "##  Confusion Matrices \n",
    "\n",
    "fig_cmatrix_lda.show()\n",
    "\n",
    "### Classification Reports \n",
    "\n",
    "print(\"Title : The Classification Report for Linear Discriminant Model: \\n  \", pred_report_lda)\n",
    "\n",
    "###  roc_auc scores \n",
    "\n",
    "print(\"The roc_auc score for Linear Discriminat Analysis Model:  \", roc_lda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K Fold Cross Validation ####\n",
    "    \n",
    "cross_valid = True\n",
    "cv_score = cross_val_score(lda_model , x_trainsc , y_os_train, scoring = 'accuracy', \n",
    "                                          cv = KFold(n_splits = 10))\n",
    "accuracy = accuracy_score(y_test, y_lda_0)\n",
    "\n",
    "print(\"The Cross Validation Score For Baseline Linear Discriminant Model After\", \"Fold Cross Validation Is :  \\n\", cv_score)\n",
    "print(\" \")\n",
    "print(\"The Accuracy Score For Baseline Linear Discriminant Model After Cross Validation Is: \\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c2432",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b65f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "\n",
    "qda_model         =      QuadraticDiscriminantAnalysis()\n",
    "\n",
    "## Model.fit\n",
    "\n",
    "qda_model.fit(x_trainsc, y_os_train)\n",
    "\n",
    "## Model.predict\n",
    "\n",
    "y_qda_0 = qda_model.predict(x_testsc)\n",
    "\n",
    "## Quadratic Discriminant Analysis Model\n",
    "\n",
    "conf_matrix_qda = confusion_matrix(y_test, y_qda_0 , labels=[0, 1])\n",
    "\n",
    "df_cmatrix_qda = pd.DataFrame(conf_matrix_qda, index = [i for i in [0, 1]],\n",
    "                  columns = [i for i in [\"Predict_Rejected\",\"Predict_Accepted\"]])\n",
    "\n",
    "\n",
    "fig_cmatrix_qda = px.imshow(df_cmatrix_qda , title = \"Confusion Matrix for Baseline Quadratic Discriminant Analysis Model\")\n",
    "\n",
    "## Saving the Classification Reports : precision, recall, f1-score ##\n",
    "\n",
    "pred_report_qda = classification_report(y_test, y_qda_0 , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Baseline Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_qda_0)\n",
    "roc_qda = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "#### Printing the Accuracy Scores \n",
    "\n",
    "print(\"The Accuracy Score For The Quadratic inear Discriminant Analysis Model Is :  \",   \n",
    "      accuracy_score(y_test, y_qda_0))\n",
    "\n",
    "##  Confusion Matrices \n",
    "\n",
    "fig_cmatrix_qda.show()\n",
    "\n",
    "### Classification Reports \n",
    "\n",
    "print(\"Title : The Classification Report for Quadratic Discriminant Model: \\n \", pred_report_qda)\n",
    "\n",
    "###  roc_auc scores \n",
    "\n",
    "print(\"The roc_auc score for Baseline Quadratic Discriminat Analysis Model:  \", roc_qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K Fold Cross Validation ####\n",
    "    \n",
    "cross_valid = True\n",
    "cv_score = cross_val_score(qda_model , x_trainsc , y_os_train, scoring = 'accuracy', \n",
    "                                          cv = KFold(n_splits = 10))\n",
    "accuracy = accuracy_score(y_test, y_qda_0)\n",
    "\n",
    "print(\"The Cross Validation Score For Baseline Quadratic Discriminant Model  After\", \"Fold Cross Validation Is :  \\n\", cv_score)\n",
    "print(\" \")\n",
    "print(\"The Accuracy Score For Baseline Quadratic Discriminant Model  After Cross Validation Is: \\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137eeea1",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ddfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "knn_model          =       KNeighborsClassifier()\n",
    "\n",
    "## Model.fit\n",
    "\n",
    "knn_model.fit(x_trainsc, y_os_train)\n",
    "\n",
    "## Model.predict\n",
    "\n",
    "y_knn_0 = knn_model.predict(x_testsc)\n",
    "\n",
    "## K- Nearest Neighbor Model\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_knn_0 , labels=[0, 1])\n",
    "\n",
    "df_cmatrix_knn = pd.DataFrame(conf_matrix_knn, index = [i for i in [0, 1]],\n",
    "                  columns = [i for i in [\"Predict_Rejected\",\"Predict_Accepted\"]])\n",
    "\n",
    "\n",
    "fig_cmatrix_knn = px.imshow(df_cmatrix_knn , title = \"Confusion Matrix for K-Nearest Neighbor Model\")\n",
    "\n",
    "## Saving the Classification Reports : precision, recall, f1-score ##\n",
    "\n",
    "pred_report_knn = classification_report(y_test, y_knn_0 , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Baseline Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_knn_0)\n",
    "roc_knn = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "#### Printing the Accuracy Scores \n",
    "\n",
    "print(\"The Accuracy Score For The K-Nearest Neighbor Analysis Model Is :  \",   \n",
    "      accuracy_score(y_test, y_knn_0))\n",
    "\n",
    "##  Confusion Matrices \n",
    "\n",
    "fig_cmatrix_knn.show()\n",
    "\n",
    "### Classification Reports \n",
    "\n",
    "print(\"Title : The Classification Report for K-Nearest Neighbor Model: \\n \", pred_report_knn)\n",
    "\n",
    "###  roc_auc scores \n",
    "\n",
    "print(\"The roc_auc score for K-Nearest Neighbor Analysis Model:  \", roc_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K Fold Cross Validation ####\n",
    "    \n",
    "cross_valid = True\n",
    "cv_score = cross_val_score(knn_model , x_trainsc , y_os_train, scoring = 'accuracy', \n",
    "                                          cv = KFold(n_splits = 10))\n",
    "accuracy = accuracy_score(y_test, y_knn_0)\n",
    "\n",
    "print(\"The Cross Validation Score For Baseline K- Nearest Neighbour Classifier Model After\", \"Fold Cross Validation Is :  \\n\", cv_score)\n",
    "print(\" \")\n",
    "print(\"The Accuracy Score For Baseline K- Nearest Neighbour Classifier Model After Cross Validation Is: \\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3518b6",
   "metadata": {},
   "source": [
    "### Support Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "svm_model =  svm.SVC()\n",
    "\n",
    "## Model.fit\n",
    "\n",
    "svm_model.fit(x_trainsc, y_os_train)\n",
    "\n",
    "## Model.predict\n",
    "\n",
    "y_svc_0 = svm_model.predict(x_testsc)\n",
    "\n",
    "## Support Vector Model\n",
    "\n",
    "conf_matrix_svc = confusion_matrix(y_test, y_svc_0 , labels=[0, 1])\n",
    "\n",
    "df_cmatrix_svc = pd.DataFrame(conf_matrix_knn, index = [i for i in [0, 1]],\n",
    "                  columns = [i for i in [\"Predict_Rejected\",\"Predict_Accepted\"]])\n",
    "\n",
    "\n",
    "fig_cmatrix_svc = px.imshow(df_cmatrix_svc , title = \"Confusion Matrix for Support Vector Model\")\n",
    "\n",
    "## Saving the Classification Reports : precision, recall, f1-score ##\n",
    "\n",
    "pred_report_svc = classification_report(y_test, y_svc_0 , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Baseline Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_svc_0)\n",
    "roc_svc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "#### Printing the Accuracy Scores \n",
    "\n",
    "print(\"The Accuracy Score For The Support Vector Analysis Model Is :  \",   \n",
    "      accuracy_score(y_test, y_svc_0))\n",
    "\n",
    "##  Confusion Matrices \n",
    "\n",
    "fig_cmatrix_svc.show()\n",
    "\n",
    "### Classification Reports \n",
    "\n",
    "print(\"Title : The Classification Report for Support Vector Model: \\n \", pred_report_svc)\n",
    "\n",
    "###  roc_auc scores \n",
    "\n",
    "print(\"The roc_auc score for Support Vector Analysis Model:  \", roc_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K Fold Cross Validation ####\n",
    "    \n",
    "cross_valid = True\n",
    "cv_score = cross_val_score(svm_model , x_trainsc , y_os_train, scoring = 'accuracy', \n",
    "                                          cv = KFold(n_splits = 10))\n",
    "accuracy = accuracy_score(y_test, y_svc_0)\n",
    "\n",
    "print(\"The Cross Validation Score For Baseline Support Vector Classifier Model After\", \"Fold Cross Validation Is :  \\n\", cv_score)\n",
    "print(\" \")\n",
    "print(\"The Accuracy Score For Baseline Support Vector Classifier Model After Cross Validation Is: \\n\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5eb17",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64242f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "logit_model = LogisticRegression()\n",
    "\n",
    "## Model.fit\n",
    "\n",
    "logit_model.fit(x_trainsc, y_os_train)\n",
    "\n",
    "## Model.predict\n",
    "\n",
    "y_logr_0 = logit_model.predict(x_testsc)\n",
    "\n",
    "## Logistic Regression Model\n",
    "\n",
    "conf_matrix_logr = confusion_matrix(y_test, y_logr_0 , labels=[0, 1])\n",
    "\n",
    "df_cmatrix_logr = pd.DataFrame(conf_matrix_knn, index = [i for i in [0, 1]],\n",
    "                  columns = [i for i in [\"Predict_Rejected\",\"Predict_Accepted\"]])\n",
    "\n",
    "\n",
    "fig_cmatrix_logr = px.imshow(df_cmatrix_logr , title = \"Confusion Matrix for Logistic Regression Model\")\n",
    "\n",
    "## Saving the Classification Reports : precision, recall, f1-score ##\n",
    "\n",
    "pred_report_logr = classification_report(y_test, y_logr_0 , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Baseline Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_logr_0)\n",
    "roc_logr = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "#### Printing the Accuracy Scores \n",
    "\n",
    "print(\"The Accuracy Score For The Logistic Regression Analysis Model Is :  \",   \n",
    "      accuracy_score(y_test, y_logr_0))\n",
    "\n",
    "##  Confusion Matrices \n",
    "\n",
    "fig_cmatrix_logr.show()\n",
    "\n",
    "### Classification Reports \n",
    "\n",
    "print(\"Title : The Classification Report for Logistic Regression Model: \\n \", pred_report_logr)\n",
    "\n",
    "###  roc_auc scores \n",
    "\n",
    "print(\"The roc_auc score for Logistic Regression Analysis Model:  \", roc_logr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### K Fold Cross Validation ####\n",
    "    \n",
    "cross_valid = True\n",
    "cv_score = cross_val_score(logit_model , x_trainsc , y_os_train, scoring = 'accuracy', \n",
    "                                          cv = KFold(n_splits = 10))\n",
    "accuracy = accuracy_score(y_test, y_logr_0)\n",
    "\n",
    "print(\"The Cross Validation Score For Baseline Logistic Regression Classifier Model After\", \"Fold Cross Validation Is :  \\n\", cv_score)\n",
    "print(\" \")\n",
    "print(\"The Accuracy Score For Baseline Logistic Regression Classifier Model After Cross Validation Is: \\n\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper-parameter Tuning of Logistic Regression\n",
    "\n",
    "penalty = ['l1', 'l2'] \n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "class_weight = ['balanced', None]\n",
    "solver = ['newton-cg','lbfgs','liblinear','sag','saga']\n",
    "\n",
    "param_grid = dict(penalty=penalty,\n",
    "                  C=C,\n",
    "                  class_weight=class_weight,\n",
    "                  solver=solver)\n",
    "\n",
    "grid = GridSearchCV(estimator = logit_model,\n",
    "                    param_grid = param_grid,\n",
    "                    scoring ='roc_auc',\n",
    "                    verbose = 1,\n",
    "                    n_jobs =-1, cv = 10)\n",
    "grid_result = grid.fit(x_trainsc, y_os_train)\n",
    "\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "logit_model = LogisticRegression(C=0.1, class_weight='balanced', penalty= 'l2', solver='liblinear')\n",
    "\n",
    "## Model.fit\n",
    "\n",
    "logit_model.fit(x_trainsc, y_os_train)\n",
    "\n",
    "## Model.predict\n",
    "\n",
    "y_logr_0 = logit_model.predict(x_testsc)\n",
    "\n",
    "## Logistic Regression Model\n",
    "\n",
    "conf_matrix_logr = confusion_matrix(y_test, y_logr_0 , labels=[0, 1])\n",
    "\n",
    "df_cmatrix_logr = pd.DataFrame(conf_matrix_knn, index = [i for i in [0, 1]],\n",
    "                  columns = [i for i in [\"Predict_Rejected\",\"Predict_Accepted\"]])\n",
    "\n",
    "\n",
    "fig_cmatrix_logr = px.imshow(df_cmatrix_logr , title = \"Confusion Matrix for Logistic Regression Model\")\n",
    "\n",
    "## Saving the Classification Reports : precision, recall, f1-score ##\n",
    "\n",
    "pred_report_logr = classification_report(y_test, y_logr_0 , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Baseline Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_logr_0)\n",
    "roc_logr = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "#### Printing the Accuracy Scores \n",
    "\n",
    "print(\"The Accuracy Score For The Logistic Regression Analysis Model Is :  \",   \n",
    "      accuracy_score(y_test, y_logr_0))\n",
    "\n",
    "##  Confusion Matrices \n",
    "\n",
    "fig_cmatrix_logr.show()\n",
    "\n",
    "### Classification Reports \n",
    "\n",
    "print(\"Title : The Classification Report for Logistic Regression Model: \\n \", pred_report_logr)\n",
    "\n",
    "###  roc_auc scores \n",
    "\n",
    "print(\"The roc_auc score for Logistic Regression Analysis Model:  \", roc_logr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c38bb8b",
   "metadata": {},
   "source": [
    "###  Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488d24c",
   "metadata": {},
   "source": [
    "Building Decision tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f68d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)\n",
    "dTree.fit(x_trainsc, y_os_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8819722",
   "metadata": {},
   "source": [
    "Scoring our Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dTree.score(x_trainsc, y_os_train))\n",
    "print(dTree.score(x_testsc, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928433d0",
   "metadata": {},
   "source": [
    "Visualizing the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff12157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "tree.plot_tree(dTree,filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d5369",
   "metadata": {},
   "source": [
    "Reducing Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth = 4, random_state=1)\n",
    "dTreeR.fit(x_trainsc, y_os_train)\n",
    "print(dTreeR.score(x_trainsc, y_os_train))\n",
    "print(dTreeR.score(x_testsc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dTreeR = DecisionTreeClassifier(criterion = 'entropy', max_depth = 4, random_state=1)\n",
    "dTreeR.fit(x_trainsc, y_os_train)\n",
    "print(dTreeR.score(x_trainsc, y_os_train))\n",
    "print(dTreeR.score(x_testsc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dTreeR.score(x_testsc , y_test))\n",
    "y_predict = dTreeR.predict(x_testsc)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict, labels=[0, 1])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"No\",\"Yes\"]],\n",
    "                  columns = [i for i in [\"No\",\"Yes\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True ,fmt='g')\n",
    "\n",
    "## Saving the Classification Reports for Meta Learning Models:\n",
    "\n",
    "pred_report_tree = classification_report(y_test, y_predict , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Meta Learning Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predict)\n",
    "roc_tree = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "## Saving the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(\"Accuracy Score for Decision Tree: \",accuracy)\n",
    "print(\"\\n\")\n",
    "print(\"Decision Tree Classifier Report\")\n",
    "print(\"\\n\")\n",
    "print(pred_report_tree)\n",
    "print(\"\\n\")\n",
    "print(\"ROC score: \",roc_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f1f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('std_slc', scalar),\n",
    "                           ('pca', pca),\n",
    "                           ('dec_tree', dTreeR)])\n",
    "\n",
    "n_components = list(range(1,x.shape[1]+1,1))\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "\n",
    "parameters = dict(pca__n_components=n_components,\n",
    "                      dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth)\n",
    "\n",
    "clf_GS = GridSearchCV(pipe, parameters)\n",
    "clf_GS.fit(x_trainsc, y_os_train)\n",
    "    \n",
    "print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\n",
    "print('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n",
    "print('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\n",
    "pca = PCA(n_components = 0.5)\n",
    "pca.fit(x_trainsc)\n",
    "x_train_model = pca.transform(x_trainsc)\n",
    "x_test_model = pca.transform(x_testsc)\n",
    "ex_variance=np.var(x_train_model,axis=0)\n",
    "ex_variance_ratio = ex_variance/np.sum(ex_variance)\n",
    "\n",
    "print(\"shape of x_train_pca\", x_train_model.shape)\n",
    "print('')    \n",
    "print(\"Explained Variance Ratio for Training Dataset: \\n\", ex_variance_ratio)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "ex_variance_1 = np.var(x_test_model , axis=0)\n",
    "ex_variance_ratio_1 = ex_variance_1 / np.sum(ex_variance_1)\n",
    "    \n",
    "print(\"Explained Variance Ratio for Test Dataset: \\n\", ex_variance_ratio_1) \n",
    "print(\" \")\n",
    "print(clf_GS.best_estimator_.get_params()['dec_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ca7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 6)\n",
    "pca.fit(x_trainsc)\n",
    "x_train_model = pca.transform(x_trainsc)\n",
    "x_test_model = pca.transform(x_testsc)\n",
    "ex_variance=np.var(x_train_model,axis=0)\n",
    "ex_variance_ratio = ex_variance/np.sum(ex_variance)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "ex_variance_1 = np.var(x_test_model , axis=0)\n",
    "ex_variance_ratio_1 = ex_variance_1 / np.sum(ex_variance_1)\n",
    "\n",
    "\n",
    "dTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth = 6, random_state=3)\n",
    "dTreeR.fit(x_trainsc, y_os_train)\n",
    "print(dTreeR.score(x_trainsc, y_os_train))\n",
    "print(dTreeR.score(x_testsc, y_test))\n",
    "\n",
    "print(dTreeR.score(x_testsc , y_test))\n",
    "y_predict = dTreeR.predict(x_testsc)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict, labels=[0, 1])\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in [\"No\",\"Yes\"]],\n",
    "                  columns = [i for i in [\"No\",\"Yes\"]])\n",
    "plt.figure(figsize = (7,5))\n",
    "sns.heatmap(df_cm, annot=True ,fmt='g')\n",
    "\n",
    "## Saving the Classification Reports for Meta Learning Models:\n",
    "\n",
    "pred_report_tree = classification_report(y_test, y_predict , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Meta Learning Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predict)\n",
    "roc_tree = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "## Saving the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(\"Accuracy Score for Decision Tree: \",accuracy)\n",
    "print(\"\\n\")\n",
    "print(\"Decision Tree Classifier Report\")\n",
    "print(\"\\n\")\n",
    "print(pred_report_tree)\n",
    "print(\"\\n\")\n",
    "print(\"ROC score: \",roc_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9676268",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54080dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model        =      RandomForestClassifier(criterion = 'gini', \n",
    "                                              n_estimators = 100, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(x_trainsc, y_os_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict     =   rf_model.predict(x_testsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa79668",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the Classification Reports for Meta Learning Models:\n",
    "\n",
    "pred_report_tree = classification_report(y_test, y_predict , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Meta Learning Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predict)\n",
    "roc_tree = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "## Saving the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(\"Accuracy Score for Random Forest: \",accuracy)\n",
    "print(\"\\n\")\n",
    "print(\"Random Forest Classifier Report\")\n",
    "print(\"\\n\")\n",
    "print(pred_report_tree)\n",
    "print(\"\\n\")\n",
    "print(\"ROC score: \",roc_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76865ac6",
   "metadata": {},
   "source": [
    "Random Forest using entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model        =      RandomForestClassifier(criterion = 'entropy', bootstrap= True, class_weight='balanced', \n",
    "                                              n_estimators = 1000, random_state=5)\n",
    "\n",
    "rf_model.fit(x_trainsc, y_os_train)\n",
    "\n",
    "y_predict     =   rf_model.predict(x_testsc)\n",
    "\n",
    "## Saving the Classification Reports for Meta Learning Models:\n",
    "\n",
    "pred_report_tree = classification_report(y_test, y_predict , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Meta Learning Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predict)\n",
    "roc_tree = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "## Saving the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(\"Accuracy Score for Random Forest: \",accuracy)\n",
    "print(\"\\n\")\n",
    "print(\"Random Forest Classifier Report\")\n",
    "print(\"\\n\")\n",
    "print(pred_report_tree)\n",
    "print(\"\\n\")\n",
    "print(\"ROC score: \",roc_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_trainsc, y_os_train)\n",
    "\n",
    "# Getting the best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e59cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model1       =      RandomForestClassifier(criterion = 'gini', bootstrap= True, class_weight='balanced',\n",
    "                                              min_samples_split= 2, max_depth=20, min_samples_leaf=2, max_features= 'sqrt',\n",
    "                                              n_estimators = 2000, random_state=10)\n",
    "\n",
    "rf_model1.fit(x_trainsc, y_os_train)\n",
    "\n",
    "y_predict     =   rf_model1.predict(x_testsc)\n",
    "\n",
    "## Saving the Classification Reports for Meta Learning Models:\n",
    "\n",
    "pred_report_tree = classification_report(y_test, y_predict , digits=2)\n",
    "\n",
    "### Saving the ROC_AUC Scores for Meta Learning Algorithms\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predict)\n",
    "roc_tree = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "## Saving the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(\"Accuracy Score for Random Forest: \",accuracy)\n",
    "print(\"\\n\")\n",
    "print(\"Random Forest Classifier Report\")\n",
    "print(\"\\n\")\n",
    "print(pred_report_tree)\n",
    "print(\"\\n\")\n",
    "print(\"ROC score: \",roc_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4a9a6",
   "metadata": {},
   "source": [
    "Boosting Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=1000)\n",
    "clf.fit(x_trainsc, y_os_train)\n",
    "y_pred = clf.predict(x_testsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Printing the Accuracy Score\n",
    "\n",
    "print(\"Accuracy Score: \",accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "########## Printing the Classification Report\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=2))\n",
    "\n",
    "### Saving the ROC_AUC Score\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc_1 = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"roc_auc_1 score: \", roc_auc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b247d74",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35430fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = GradientBoostingClassifier(n_estimators=1000)\n",
    "clf1.fit(x_trainsc, y_os_train)\n",
    "y_pred = clf1.predict(x_testsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00519f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Printing the Accuracy Score\n",
    "\n",
    "print(\"Accuracy Score: \",accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "########## Printing the Classification Report\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits=2))\n",
    "\n",
    "### Saving the ROC_AUC Score\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc_2 = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"roc_auc_2 score: \", roc_auc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model as a pickle string.\n",
    "saved_model = pickle.dumps(clf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400fb0e5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Gradient Boosting Classifier is the best model as it offers the best recall Value. Since it is a matter of life our recall is more crucial than precision. Determining survived passenger is more important. \n",
    "\n",
    "The accuracy for this model : 84.38%\n",
    "Area covered: 83.05%\n",
    "Recall for not survived: 89%\n",
    "Recall for survived: 77%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
